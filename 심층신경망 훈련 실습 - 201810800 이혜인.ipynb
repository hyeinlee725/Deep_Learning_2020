{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "심층신경망 훈련 실습\n",
    "================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 휴먼지능정보공학과 201810800 이혜인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10을 CNN으로 학습하기. 여러 학습 방법으로 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (8) learning rate decay\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "batch_size=16\n",
    "learning_rate=0.002\n",
    "num_epoch=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CIFAR10 train, test dataset 가져오기(163 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MNIST : 11MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cifar_train=dset.CIFAR10(\"CIFAR10/\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "cifar_test=dset.CIFAR10(\"CIFAR10/\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 대략적인 데이터 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar_train 길이:"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cifar_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7654f9d7df77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"cifar_train 길이:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"cifar_test 길이:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 데이터 하나 형태\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1번째 데이터\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cifar_train' is not defined"
     ]
    }
   ],
   "source": [
    "print \"cifar_train 길이:\", len(cifar_train)\n",
    "print \"cifar_test 길이:\", len(cifar_test)\n",
    "\n",
    "# 데이터 하나 형태\n",
    "image, label = cifar_train.__getitem__(1) # 1번째 데이터\n",
    "print \"image data 형태:\", image.size()\n",
    "print \"label:\", label\n",
    "\n",
    "# 그리기\n",
    "img = image.numpy() # image 타입을 numpy로 변환 (3, 32, 32)\n",
    "\n",
    "# (3, 32, 32) -> (32, 32, 3)\n",
    "r, g, b = img[0,:,:], img[1,:,:], img[2,:,:]\n",
    "#img = img.reshape(img.shape[1], img.shape[2], img.shape[0])\n",
    "img2 = np.zeros((img.shape[1], img.shape[2], img.shape[0]))\n",
    "img2[:,:,0], img2[:,:,1], img2[:,:,2] = r,g,b\n",
    "\n",
    "plt.title(\"label: %d\" %label)\n",
    "plt.imshow(img2, interpolation = 'bicubic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccr(dloader, imodel):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for j, [imgs, labels] in enumerate(dloader): # batch_size 만큼\n",
    "        img = Variable(imgs, volatile = True).cuda() # x\n",
    "        #label = Variable(labels) # y\n",
    "        label = Variable(labels).cuda()\n",
    "        # .cuda() : GPI에 로드되기 위함. 만약 CPU로 설정되어 있다면 에러남\n",
    "        \n",
    "        output = imodel.forward(img) #forward prop.\n",
    "        _, output_index = torch.max(output, 1)\n",
    "        \n",
    "        total += label.size(0)\n",
    "        correct += (output_index == label).sum().float()\n",
    "    print (\"Accuracy of Test Data: {}\".format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. 데이터 로드함수 ===\n",
    "train_loader = torch.utils.data.DataLoader(list(cifar_train)[:], batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(cifar_test, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# === 4. 모델 선언 ===\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer=nn.Sequential(\n",
    "            nn.Conv2d(3,16,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),   # (2) drop out\n",
    "            #nn.BatchNorm2d(16),  # (6) Batch normalization\n",
    "            nn.Conv2d(16,32,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32,64,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),\n",
    "            #nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(64*8*8, 100),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2(0,2),\n",
    "            #nn.BatchNorm2d(100),\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.layer(x)\n",
    "        out=out.view(batch_size, -1)\n",
    "        out=self.fc_layer(out)\n",
    "        \n",
    "        return out\n",
    "model=CNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. loss, optimizer ===\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# === 6. 학습 ===\n",
    "for i in range(num_epoch):\n",
    "    for j, [image, label] in enumerate(train_loader):\n",
    "        x=Variable(image).cuda()\n",
    "        y_=Variable(label).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output=model.forward(x)\n",
    "        loss=loss_func(output, y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j%1000==0:\n",
    "            print(j, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) Naive Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeAccr(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. 데이터 로드함수 ===\n",
    "train_loader = torch.utils.data.DataLoader(list(cifar_train)[:], batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(cifar_test, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# === 4. 모델 선언 ===\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer=nn.Sequential(\n",
    "            nn.Conv2d(3,16,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0,2),   # (2) drop out\n",
    "            #nn.BatchNorm2d(16),  # (6) Batch normalization\n",
    "            nn.Conv2d(16,32,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0,2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32,64,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0,2),\n",
    "            #nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(64*8*8, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2(0,2),\n",
    "            #nn.BatchNorm2d(100),\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.layer(x)\n",
    "        out=out.view(batch_size, -1)\n",
    "        out=self.fc_layer(out)\n",
    "        \n",
    "        return out\n",
    "model=CNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeAccr(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. 데이터 로드함수 ===\n",
    "train_loader = torch.utils.data.DataLoader(list(cifar_train)[:], batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(cifar_test, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# === 4. 모델 선언 ===\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer=nn.Sequential(\n",
    "            nn.Conv2d(3,16,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),   # (2) drop out\n",
    "            #nn.BatchNorm2d(16),  # (6) Batch normalization\n",
    "            nn.Conv2d(16,32,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32,64,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),\n",
    "            #nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(64*8*8, 100),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2(0,2),\n",
    "            #nn.BatchNorm2d(100),\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.layer(x)\n",
    "        out=out.view(batch_size, -1)\n",
    "        out=self.fc_layer(out)\n",
    "        \n",
    "        return out\n",
    "model=CNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cifar_train=dset.CIFAR10(\"CIFAR10/\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "# (2) Data augmentation\n",
    "cifar_train=dset.CIFAR10(\"CIFAR10/\", train=True, \n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.Scale(36),\n",
    "                            transforms.CenterCrop(32),\n",
    "                            transforms.RandomHorizontalFlip(),\n",
    "                            transforms.Lambda(lambda x: x.rotate(90)),\n",
    "                            transforms.ToTensor()\n",
    "                        ]))\n",
    "\n",
    "cifar_test=dset.CIFAR10(\"CIFAR10/\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeAccr(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. 데이터 로드함수 ===\n",
    "train_loader = torch.utils.data.DataLoader(list(cifar_train)[:], batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(cifar_test, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# === 4. 모델 선언 ===\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer=nn.Sequential(\n",
    "            nn.Conv2d(3,16,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),   # (2) drop out\n",
    "            #nn.BatchNorm2d(16),  # (6) Batch normalization\n",
    "            nn.Conv2d(16,32,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32,64,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),\n",
    "            #nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(64*8*8, 100),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2(0,2),\n",
    "            #nn.BatchNorm2d(100),\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "        \n",
    "        # (3) weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal(m.weight.data) #RELU 일 때\n",
    "                m.bias.data.fill_(0)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal(m.weight.data)\n",
    "                m.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.layer(x)\n",
    "        out=out.view(batch_size, -1)\n",
    "        out=self.fc_layer(out)\n",
    "        \n",
    "        return out\n",
    "model=CNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeAccr(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. 데이터 로드함수 ===\n",
    "train_loader = torch.utils.data.DataLoader(list(cifar_train)[:], batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(cifar_test, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# === 4. 모델 선언 ===\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer=nn.Sequential(\n",
    "            nn.Conv2d(3,16,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),   # (2) drop out\n",
    "            #nn.BatchNorm2d(16),  # (6) Batch normalization\n",
    "            nn.Conv2d(16,32,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32,64,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),\n",
    "            #nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(64*8*8, 100),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2(0,2),\n",
    "            #nn.BatchNorm2d(100),\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.layer(x)\n",
    "        out=out.view(batch_size, -1)\n",
    "        out=self.fc_layer(out)\n",
    "        \n",
    "        return out\n",
    "model=CNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cifar_train=dset.CIFAR10(\"CIFAR10/\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "#cifar_test=dset.CIFAR10(\"CIFAR10/\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "# (2) Data augmentation\n",
    "cifar_test=dset.CIFAR10(\"CIFAR10/\", train=True, \n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "                        ])\n",
    "                        , target_transform=None, download=False)\n",
    "\n",
    "cifar_test=dset.CIFAR10(\"CIFAR10/\", train=False, \n",
    "                        transform=transforms.Compose([\n",
    "                            reansforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "                        ]\n",
    "                        ), target_transform=None, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeAccr(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. 데이터 로드함수 ===\n",
    "train_loader = torch.utils.data.DataLoader(list(cifar_train)[:], batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(cifar_test, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# === 4. 모델 선언 ===\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer=nn.Sequential(\n",
    "            nn.Conv2d(3,16,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),   # (2) drop out\n",
    "            nn.BatchNorm2d(16),  # (6) Batch normalization\n",
    "            nn.Conv2d(16,32,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32,64,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(64*8*8, 100),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2(0,2),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.layer(x)\n",
    "        out=out.view(batch_size, -1)\n",
    "        out=self.fc_layer(out)\n",
    "        \n",
    "        return out\n",
    "model=CNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeAccr(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. 데이터 로드함수 ===\n",
    "train_loader = torch.utils.data.DataLoader(list(cifar_train)[:], batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(cifar_test, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# === 4. 모델 선언 ===\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer=nn.Sequential(\n",
    "            nn.Conv2d(3,16,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),   # (2) drop out\n",
    "            #nn.BatchNorm2d(16),  # (6) Batch normalization\n",
    "            nn.Conv2d(16,32,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32,64,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0,2),\n",
    "            #nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(64*8*8, 100),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2(0,2),\n",
    "            #nn.BatchNorm2d(100),\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.layer(x)\n",
    "        out=out.view(batch_size, -1)\n",
    "        out=self.fc_layer(out)\n",
    "        \n",
    "        return out\n",
    "model=CNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. loss, optimizer ===\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "#(6) Adam optimizer\n",
    "\n",
    "# === 6. 학습 ===\n",
    "for i in range(num_epoch):\n",
    "    for j, [image, label] in enumerate(train_loader):\n",
    "        x=Variable(image).cuda()\n",
    "        y_=Variable(label).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output=model.forward(x)\n",
    "        loss=loss_func(output, y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j%1000==0:\n",
    "            print(j, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. loss, optimizer ===\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "#optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "#(6) Adam optimizer\n",
    "schedule = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.2) \n",
    "# (7)learning rate\n",
    "\n",
    "# === 6. 학습 ===\n",
    "for i in range(num_epoch):\n",
    "    for j, [image, label] in enumerate(train_loader):\n",
    "        x=Variable(image).cuda()\n",
    "        y_=Variable(label).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output=model.forward(x)\n",
    "        loss=loss_func(output, y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j%1000==0:\n",
    "            print(j, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeAccr(test_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p27",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
